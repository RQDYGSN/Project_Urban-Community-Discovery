{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import community\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from modularity_maximization import partition\n",
    "from modularity_maximization.utils import get_modularity\n",
    "from modutils import mod_calc\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "gpus = [0]\n",
    "from typing import Dict\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = random.randint(5, 10)\n",
    "M = random.randint(1, 100)\n",
    "average_cluster_size = random.randint(20, 40)\n",
    "center_x_list = []\n",
    "center_y_list = []\n",
    "for i in range(K):\n",
    "    center_x_list.append(round(10*random.random(), 4))\n",
    "    center_y_list.append(round(10*random.random(), 4))\n",
    "node_groundtruth = {}\n",
    "node_location = {}\n",
    "idx = 0\n",
    "for i in range(K):\n",
    "    for j in range(average_cluster_size):\n",
    "        node_groundtruth[idx] = i\n",
    "        node_location[idx] = [center_x_list[i] + round(random.gauss(0, 1), 4), center_y_list[i] + round(random.gauss(0, 1), 4)]\n",
    "        idx = idx + 1\n",
    "N = len(node_groundtruth)\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(list(range(N)))\n",
    "loc_np = np.array(list(node_location.values()))\n",
    "x_np = loc_np[:, 0].reshape((1, -1))\n",
    "y_np = loc_np[:, 1].reshape((1, -1))\n",
    "distance_np = np.square(x_np - x_np.T)+np.square(y_np - y_np.T)\n",
    "k_nearest = {}\n",
    "k = int(average_cluster_size/2)\n",
    "for i in range(N):\n",
    "    k_near_pre = list(np.argsort(distance_np[i])[:k+1:])\n",
    "    k_near_pre.remove(i)\n",
    "    k_nearest[i] = k_near_pre\n",
    "for i in range(N):\n",
    "    for j in k_nearest[i]:\n",
    "        G.add_edge(i, j, weight = np.exp(-1/2*distance_np[i, j]))\n",
    "G.to_undirected()\n",
    "omega = 1/K*np.ones(K)\n",
    "beta = 1/M*np.ones(M)\n",
    "FF = omega.reshape((1, -1))*beta.reshape((-1, 1))\n",
    "FF = FF*K*M\n",
    "X = np.zeros((N, M))\n",
    "for i in range(M):\n",
    "    for j in range(K):\n",
    "        tmp = np.random.dirichlet(np.ones(average_cluster_size), size=1)*FF[i, j]\n",
    "        X[j*average_cluster_size:(j+1)*average_cluster_size:, i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDRAE(nn.Module):\n",
    "    def __init__(self, A_hat, num_feat, num_hidden, W_prior, weight_list):\n",
    "        super(CDRAE, self).__init__()\n",
    "        f = nn.Softmax(dim=2)\n",
    "        self.graph_num = A_hat.shape[0]\n",
    "        self.num_feat = num_feat # 特征数 f\n",
    "        self.num_hidden = num_hidden # 隐含数 h\n",
    "        self.weight_list = weight_list\n",
    "        self.A_hat = A_hat\n",
    "        self.d = self.A_hat.sum(axis=1).float().contiguous().view(self.graph_num, -1, 1)\n",
    "        self.dT = self.A_hat.sum(axis=2).float().contiguous().view(self.graph_num, -1, 1)\n",
    "        self.ddT = torch.mul(self.d.expand(self.graph_num, self.num_feat, self.num_feat), self.dT.expand(self.graph_num, self.num_feat, self.num_feat))\n",
    "        self.B = (self.A_hat - torch.div(self.ddT, torch.sum(torch.sum(self.A_hat, axis=-1), axis=-1).view(-1, 1, 1))).float()\n",
    "        self.B_norm = torch.div(self.B, torch.sum(torch.sum(self.A_hat, axis=-1), axis=-1).view(-1, 1, 1))\n",
    "        self.W_0 = nn.Parameter(f(torch.ones(self.graph_num, self.num_feat, self.num_hidden).to(device)+W_prior)) # [1][f*h]\n",
    "\n",
    "    def forward(self, A_hat,temp, epoch): # X貌似暂时没有用到\n",
    "        global featureSelector # \n",
    "        global weight_feature # \n",
    "        \n",
    "        featureSelector = self.W_0 # [1][f*h]\n",
    "        weight_feature_multi = torch.zeros(self.W_0.size()).to(device)\n",
    "        x = 1*(epoch+1) # x次取平均,目测这个目的是为了稳定。\n",
    "        for j in range(self.graph_num):\n",
    "            results = torch.zeros((self.num_feat, self.num_hidden)).to(device) # [0][f*h]\n",
    "            for i in range(x):\n",
    "                results += F.gumbel_softmax(self.W_0[j],tau=temp,hard=True)\n",
    "            weight_feature_multi[j] = results/x # [-][f*h]\n",
    "        weight_feature = torch.mean(weight_feature_multi, axis=0)\n",
    "        \n",
    "        H = torch.mm(torch.mm(weight_feature.T, torch.mean(self.A_hat*self.weight_list.view(-1,1,1), axis=0)), weight_feature) # weight_feature 就是我们要求的U\n",
    "        H = torch.div(H, H.sum(axis=0)) # 按照每一列进行归一化，列为axis=0的方向。\n",
    "        m = nn.Softmax(dim=0)\n",
    "        return m(H)\n",
    "    \n",
    "def lossFn(output): \n",
    "    return torch.trace(-torch.log(output))\n",
    "\n",
    "def th_kl_div(p, q):\n",
    "    return torch.mean(torch.sum((0.00001+p)*torch.log(torch.div(0.00001+p, 0.00001+q)), axis=1))*p.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_number=K\n",
    "feature=X\n",
    "multi_graph=[G]\n",
    "alpha_1=1\n",
    "alpha_2=0.5\n",
    "alpha_3=0.01\n",
    "weight_list=[1]\n",
    "alpha=2\n",
    "epoch_num=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "DG = dgl.DGLGraph()\n",
    "DG.from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Creating a local scope so that all the stored ndata and edata\n",
    "        # (such as the `'h'` ndata below) are automatically popped out\n",
    "        # when the scope exits.\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = feature\n",
    "            g.update_all(gcn_msg, gcn_reduce)\n",
    "            h = g.ndata['h']\n",
    "            return self.linear(h)\n",
    "        \n",
    "class GCN_model(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCN_model, self).__init__()\n",
    "        self.layer = GCNLayer(in_feats, out_feats)\n",
    "        # self.B = B\n",
    "\n",
    "    def forward(self, g, features, beta):\n",
    "        global gcn_output\n",
    "        n = nn.Softmax(dim=1)\n",
    "        \n",
    "        gcn_output = self.layer(g, features)\n",
    "        weight_feature = n(gcn_output)\n",
    "        W_noise = torch.div(torch.exp(weight_feature)-1, torch.exp(weight_feature)+1)*beta\n",
    "        return W_noise\n",
    "    \n",
    "class GCN_prior(nn.Module):\n",
    "    def __init__(self, A_hat, num_node, num_feat, num_cluster):\n",
    "        super(GCN_prior, self).__init__()\n",
    "        self.num_node = num_node # 特征数 f\n",
    "        self.num_feat = num_feat\n",
    "        self.num_cluster = num_cluster # 隐含数 h\n",
    "        self.A_hat = A_hat\n",
    "        self.d = torch.tensor(list(dict(G.degree()).values()), dtype=np.float).contiguous().view(-1, 1)\n",
    "        self.ddT = torch.mm(self.d, self.d.T)\n",
    "        self.B = (self.A_hat - torch.div(self.ddT, torch.norm(self.A_hat, p=1))).float()\n",
    "        self.B_norm = torch.div(self.B, torch.norm(self.A_hat, p=1))\n",
    "        self.W_0 = nn.Parameter(torch.ones(num_node, num_cluster)) # [1][f*h]\n",
    "        self.layer = GCNLayer(num_feat, num_cluster)\n",
    "\n",
    "    def forward(self, X, DG, temp, epoch): # X貌似暂时没有用到\n",
    "        global featureSelector # \n",
    "        global weight_feature # \n",
    "        # f = nn.Softmax(dim=1)\n",
    "        gcn_output = self.layer(DG, X)\n",
    "        featureSelector = self.W_0 + gcn_output # [1][f*h]\n",
    "        results = torch.zeros(featureSelector.size()) # [0][f*h]\n",
    "        x = 1*(epoch+1) # x次取平均,目测这个目的是为了稳定。\n",
    "        for i in range(x):\n",
    "            # logits --> self.W_0: [batch_size, num_features] 非规范化对数概率\n",
    "            # tau: 非负的对抗强度。\n",
    "            # hard: 如果 True, 返回的样本将会离散为 one-hot 向量\n",
    "            results += F.gumbel_softmax(featureSelector,tau=temp,hard=False)\n",
    "        weight_feature = results/x # [-][f*h]\n",
    "\n",
    "        # H = torch.mm(torch.mm(weight_feature.T, B), weight_feature) # weight_feature 就是我们要求的U\n",
    "        H = torch.mm(torch.mm(weight_feature.T, self.A_hat), weight_feature) # weight_feature 就是我们要求的U\n",
    "        Hh = torch.div(H, H.sum(axis=0)) # 按照每一列进行归一化，列为axis=0的方向。\n",
    "        m = nn.Softmax(dim=0)\n",
    "        return m(Hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 ; Loss:  11.8325 ; Modulariy:  0.6236\n",
      "Epoch:  20 ; Loss:  11.8329 ; Modulariy:  0.6252\n",
      "Epoch:  40 ; Loss:  11.8295 ; Modulariy:  0.6252\n",
      "Epoch:  60 ; Loss:  11.829 ; Modulariy:  0.6252\n",
      "Epoch:  80 ; Loss:  12.2256 ; Modulariy:  0.6206\n",
      "Epoch:  100 ; Loss:  12.0092 ; Modulariy:  0.6252\n",
      "Epoch:  120 ; Loss:  11.9589 ; Modulariy:  0.6252\n",
      "Epoch:  140 ; Loss:  11.8628 ; Modulariy:  0.6252\n",
      "Epoch:  160 ; Loss:  11.8446 ; Modulariy:  0.6252\n",
      "Epoch:  180 ; Loss:  11.8297 ; Modulariy:  0.6252\n",
      "Epoch:  200 ; Loss:  11.8297 ; Modulariy:  0.6252\n",
      "Epoch:  220 ; Loss:  11.8296 ; Modulariy:  0.6252\n",
      "Epoch:  240 ; Loss:  11.8296 ; Modulariy:  0.6252\n",
      "Epoch:  260 ; Loss:  11.8284 ; Modulariy:  0.6252\n",
      "Epoch:  280 ; Loss:  11.8285 ; Modulariy:  0.6252\n",
      "Epoch:  300 ; Loss:  11.8281 ; Modulariy:  0.6252\n",
      "Epoch:  320 ; Loss:  11.828 ; Modulariy:  0.6252\n",
      "Epoch:  340 ; Loss:  11.828 ; Modulariy:  0.6252\n",
      "Epoch:  360 ; Loss:  11.8281 ; Modulariy:  0.6252\n",
      "Epoch:  380 ; Loss:  11.8281 ; Modulariy:  0.6252\n",
      "Epoch:  400 ; Loss:  11.828 ; Modulariy:  0.6252\n",
      "Epoch:  420 ; Loss:  11.828 ; Modulariy:  0.6252\n",
      "Epoch:  440 ; Loss:  11.828 ; Modulariy:  0.6252\n",
      "Epoch:  460 ; Loss:  11.828 ; Modulariy:  0.6252\n",
      "Epoch:  480 ; Loss:  11.828 ; Modulariy:  0.6252\n",
      "Epoch:  499 ; Loss:  11.828 ; Modulariy:  0.6252\n"
     ]
    }
   ],
   "source": [
    "W_prior_list = []\n",
    "\n",
    "for G in multi_graph:\n",
    "\n",
    "    A = torch.from_numpy(np.array(nx.adj_matrix(G).todense())).clone().float()\n",
    "    features = torch.from_numpy(feature).clone().float()\n",
    "    num_node = len(G.nodes()) # f = 节点数\n",
    "    num_feat = features.size()[1]\n",
    "    num_cluster = clusters_number # h = 聚类数\n",
    "    model_base = GCN_prior(A, num_node, num_feat, num_cluster)\n",
    "    optimizer_base = optim.Adam(model_base.parameters(),lr=1.5e-2)\n",
    "    temp = 3\n",
    "    epoch_prior_num = 200\n",
    "\n",
    "    for epoch in range(epoch_prior_num):\n",
    "        model_base.train()\n",
    "        model_base.zero_grad()\n",
    "        if(epoch == 75):\n",
    "            temp = 2.75\n",
    "        elif(epoch == 100):\n",
    "            temp = 2.5\n",
    "        elif(epoch == 125):\n",
    "            temp = 2\n",
    "        elif(epoch == 150):\n",
    "            temp = 1.8\n",
    "        elif(epoch == 175):\n",
    "            temp = 1.25\n",
    "        elif(epoch == 250):\n",
    "            temp = 1.00\n",
    "        elif(epoch == 300):\n",
    "            temp = 0.75\n",
    "        elif(epoch == 320):\n",
    "            temp = 0.50\n",
    "        elif(epoch == 400):\n",
    "            temp = 0.20\n",
    "        output = model_base(features, DG, temp, epoch)\n",
    "\n",
    "        loss = lossFn(output)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer_base.step()\n",
    "\n",
    "    gumbel_matrix = weight_feature.detach().max(dim=1)[1] # 每个节点的标签组成的tensor\n",
    "    labels_pred = gumbel_matrix.data.numpy()\n",
    "    # modu = community.modularity(dict(zip(list(range(len(labels_pred))), labels_pred)), G, weight='weight')\n",
    "    # print('Epoch: ', epoch, '; Loss: ', round(loss.item(), 4), '; Modulariy: ', round(modu, 4))\n",
    "    \n",
    "    # 填装 prior\n",
    "    partition = dict(zip(list(range(len(labels_pred))), labels_pred))\n",
    "    partition_count = {}\n",
    "    for key, value in partition.items():\n",
    "        if value not in partition_count:\n",
    "            partition_count[value] = 1\n",
    "        else:\n",
    "            partition_count[value] = partition_count[value] + 1\n",
    "\n",
    "    # print(community.modularity(partition, G, weight='weight'))\n",
    "    remapping = {}\n",
    "    idx = 0\n",
    "    for key, value in partition_count.items():\n",
    "        if value>1 and key not in remapping:\n",
    "            remapping[key] = idx\n",
    "            idx = idx + 1\n",
    "\n",
    "    tmp = np.zeros((len(G), clusters_number))\n",
    "    for key, value in partition.items():\n",
    "        try:\n",
    "            tmp[key, remapping[value]] = alpha\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    W_prior_list.append(tmp)\n",
    "\n",
    "# 组合 prior\n",
    "for i in range(len(W_prior_list)):\n",
    "    W_prior_list[i] = torch.tensor(W_prior_list[i]).float().unsqueeze(0)\n",
    "W_prior = torch.cat(tuple(W_prior_list), 0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = torch.tensor(weight_list).float()\n",
    "weight_list = torch.div(weight_list, torch.sum(weight_list))\n",
    "weight_list = weight_list.to(device)\n",
    "\n",
    "num_feat = len(multi_graph[0].nodes()) # f = 节点数\n",
    "num_hidden = clusters_number # h = 聚类数\n",
    "\n",
    "A_list = []\n",
    "for i in multi_graph:\n",
    "    A_list.append(torch.tensor(nx.adjacency_matrix(i).todense(), dtype=np.float).unsqueeze(0))\n",
    "A_hat = A_hat = torch.cat(tuple(A_list), 0) # a matrix: A\n",
    "A_hat_tensor = torch.Tensor(A_hat.float()).to(device)\n",
    "\n",
    "vector_beta = torch.ones((feature.shape[1], 1)).float().to(device)\n",
    "vector_omega = 1.0/clusters_number*torch.ones((clusters_number, 1)).float().to(device)\n",
    "feature = torch.tensor(feature).float().to(device)\n",
    "\n",
    "model = CDRAE(A_hat_tensor, num_feat, num_hidden, W_prior, weight_list).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "temp = 3\n",
    "for epoch in range(epoch_num):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    if(epoch == 75):\n",
    "        temp = 2.75\n",
    "    elif(epoch == 100):\n",
    "        temp = 2.5\n",
    "    elif(epoch == 125):\n",
    "        temp = 2\n",
    "    elif(epoch == 150):\n",
    "        temp = 1.8\n",
    "    elif(epoch == 175):\n",
    "        temp = 1.25\n",
    "    elif(epoch == 250):\n",
    "        temp = 1.00\n",
    "    elif(epoch == 300):\n",
    "        temp = 0.75\n",
    "    elif(epoch == 320):\n",
    "        temp = 0.50\n",
    "    elif(epoch == 400):\n",
    "        temp = 0.20\n",
    "    output = model(A_hat_tensor, temp, epoch)\n",
    "    equality_o = torch.mm(torch.mm(weight_feature.T, feature),vector_beta)\n",
    "    equality = equality_o/equality_o.sum()\n",
    "    if epoch%10 == 0:\n",
    "        P = torch.div(torch.square(weight_feature), torch.square(weight_feature).sum(axis=1).view(-1, 1))\n",
    "    loss_clus = th_kl_div(weight_feature, P)\n",
    "    # loss_pena = torch.sum(torch.mean(torch.square(featureSelector - torch.mean(featureSelector, axis=0).unsqueeze(0)), axis=0))\n",
    "    loss_cons = torch.sqrt(torch.mean(torch.abs(equality - vector_omega))) # th_kl_div(equality, vector_omega)\n",
    "    loss = alpha_1*lossFn(output) + alpha_2*loss_cons + alpha_3*loss_clus\n",
    "\n",
    "    gumbel_matrix = weight_feature.clone().detach().max(dim=1)[1] # 每个节点的标签组成的tensor\n",
    "    labels_pred = gumbel_matrix.cpu().data.numpy()\n",
    "    ddd = dict(zip(list(range(len(labels_pred))), labels_pred))\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "######################################################################################################\n",
    "gumbel_matrix = weight_feature.clone().detach().max(dim=1)[1] # 每个节点的标签组成的tensor\n",
    "labels_pred = gumbel_matrix.cpu().data.numpy()\n",
    "partition = dict(zip(list(range(len(labels_pred))), list(labels_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return partition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
